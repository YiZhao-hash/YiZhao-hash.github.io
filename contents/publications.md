-* is equa 

-J. Zhang*, <strong>Y. Zhao*</strong>, D. Chen, X. Tian, H. Zheng, and W. Zhu (2024). MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning. EMNLP 2024 Findings. [[Paper]](https://aclanthology.org/2024.findings-emnlp.994.pdf)

-X. Tian*, <strong>Y. Zhao*</strong>, C. Yin, W. Zhu, A. X. Tian, and Y. Ge (2024). FanLoRA: Fantastic LoRAs and Where to Find Them in Large Language Model Fine-tuning. EMNLP 2024 Industry Track. [[Paper]](https://aclanthology.org/2024.emnlp-industry.38.pdf)

-Z. Liu*, <strong>Y. Zhao*</strong>, M. Tan, W. Zhu, and A. X. Tian (2024). PARA: Parameter-Efficient Fine-tuning with Prompt-Aware Representation Adjustment. EMNLP 2024 Industry Track. [[Paper]](https://aclanthology.org/2024.emnlp-industry.55.pdf)

-Y. Chen, H. Wang, S. Yan, S. Liu, Y. Li,<strong>Y. Zhao</strong>, and Y. Xiao (2024). Emotionqueen: A Benchmark for Evaluating Empathy of Large Language Models. ACL 2024 Findings. [[Paper]](https://aclanthology.org/2024.findings-acl.128.pdf)